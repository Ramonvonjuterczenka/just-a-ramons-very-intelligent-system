# ğŸ³ JARVIS Docker + Ollama Setup Guide

## ğŸ“‹ Ãœberblick

JARVIS kann jetzt **vollstÃ¤ndig mit Docker** deployed werden und funktioniert **out-of-the-box** mit:

âœ… **Ollama** - Lokales LLM (Sprachmodell)  
âœ… **Mistral** - Hochwertiges Default-Modell  
âœ… **Coqui TTS** - Hochwertige Sprachausgabe  
âœ… **Browser STT** - Spracherkennung im Browser  
âœ… **Voice Activation** - Kontinuierliches ZuhÃ¶ren  

---

## ğŸš€ Quickstart mit Docker

### 1. Docker & Docker Compose installieren

**Windows:**
```bash
# Installiere Docker Desktop
https://www.docker.com/products/docker-desktop
```

**Linux:**
```bash
sudo apt-get install docker.io docker-compose
sudo usermod -aG docker $USER
```

**macOS:**
```bash
brew install docker docker-compose
# oder Docker Desktop installieren
```

### 2. Projekt klonen/Ã¶ffnen
```bash
cd C:\GIT\jarvis  # oder dein Projekt-Pfad
```

### 3. Docker Container starten
```bash
docker-compose up --build
```

Das ist alles! ğŸ‰

### 4. Browser Ã¶ffnen
```
http://localhost:8080
```

### 5. Mikrofon erlauben & sprechen
```
"JARVIS, guten Morgen!"
```

âœ… **JARVIS antwortet mit Coqui TTS in hoher QualitÃ¤t!**

---

## ğŸ“Š Was wird installiert

### Service: Ollama
```
Docker Image: ollama/ollama:latest
Port: 11434 (intern) â†’ nicht sichtbar
Volume: jarvis_ollama_data (persistent)
Model: Mistral (7B, ~4GB)
```

### Service: JARVIS Server
```
Docker Image: Gebaut aus Dockerfile (Java + Spring Boot)
Port: 8080 â†’ http://localhost:8080
Environment:
  - STT: browser (Web Speech API)
  - TTS: coqui (hochwertige Synthese)
  - LLM: ollama (Mistral 7B)
```

---

## ğŸ¤– Ollama Models Auswahl

### â­ Standard: Mistral (EMPFOHLEN)
```
GrÃ¶ÃŸe: ~4GB
QualitÃ¤t: â­â­â­â­â­
Geschwindigkeit: â­â­â­â­
RAM: 8GB empfohlen
Anwendung: Allgemein, sehr gut
```

**Mistral ist perfekt fÃ¼r JARVIS!**

### Alternativen (optional zum Austauschen)

#### Neural Chat
```
GrÃ¶ÃŸe: ~4GB
QualitÃ¤t: â­â­â­â­â­
Geschwindigkeit: â­â­â­â­
Spezialisierung: Conversation/Chat
```

#### Dolphin Mixtral
```
GrÃ¶ÃŸe: ~26GB
QualitÃ¤t: â­â­â­â­â­â­
Geschwindigkeit: â­â­â­
RAM: 32GB empfohlen
Spezialisierung: Hochwertig, vielseitig
```

#### TinyLlama
```
GrÃ¶ÃŸe: ~600MB
QualitÃ¤t: â­â­â­
Geschwindigkeit: â­â­â­â­â­
RAM: 4GB
Spezialisierung: Schnell & leicht
```

#### Llama2
```
GrÃ¶ÃŸe: ~4GB
QualitÃ¤t: â­â­â­â­
Geschwindigkeit: â­â­â­â­
RAM: 8GB
Spezialisierung: Allgemein, zuverlÃ¤ssig
```

---

## ğŸ”§ Ollama Model wechseln

### Option 1: Vor dem Start (einfachste)
Bearbeite `docker-compose.yml`:

```yaml
ollama:
  environment:
    - OLLAMA_DEFAULT_MODEL=mistral  # â† Hier Ã¤ndern
```

Beispiele:
```yaml
- OLLAMA_DEFAULT_MODEL=neural-chat
- OLLAMA_DEFAULT_MODEL=dolphin-mixtral
- OLLAMA_DEFAULT_MODEL=tinyllama
- OLLAMA_DEFAULT_MODEL=llama2
```

Starte dann:
```bash
docker-compose up --build
```

### Option 2: Nach dem Start (while running)
```bash
# Container-ID oder -Name anzeigen
docker ps | grep ollama

# In den Container gehen
docker exec -it jarvis-ollama bash

# Neues Modell pullen
ollama pull neural-chat

# Settings Ã¤ndern
docker-compose.yml anpassen
docker-compose restart jarvis-server
```

### Option 3: Dynamisch via API
```bash
# Checke welche Modelle verfÃ¼gbar sind
curl http://localhost:11434/api/tags

# Pullt neues Modell
curl -X POST http://localhost:11434/api/pull \
  -d '{"name": "neural-chat"}'
```

---

## ğŸ’¾ Persistent Storage

Ollama-Daten werden in einem Docker Volume gespeichert:

```bash
# Volume anzeigen
docker volume ls | grep jarvis

# Volume inspizieren
docker volume inspect jarvis_ollama_data

# Modelle sehen
docker exec jarvis-ollama ollama list
```

**Die heruntergeladenen Modelle bleiben zwischen Reboots erhalten!**

---

## ğŸ” Logs & Debugging

### Logs anzeigen
```bash
# Alle Services
docker-compose logs

# Nur Ollama
docker-compose logs ollama

# Nur JARVIS
docker-compose logs jarvis-server

# Live-Logs (follow)
docker-compose logs -f jarvis-server
```

### Container-Shell
```bash
# JARVIS Container
docker exec -it jarvis-server bash

# Ollama Container
docker exec -it jarvis-ollama bash
```

### Modelle prÃ¼fen
```bash
docker exec jarvis-ollama ollama list
```

---

## ğŸ›‘ Stoppen & Neustarten

### Alle Container stoppen
```bash
docker-compose down
```

### Alle Container stoppen & Volumes lÃ¶schen
```bash
docker-compose down -v
```

### Neustart
```bash
docker-compose restart
```

### Rebuild (bei Code-Ã„nderungen)
```bash
docker-compose up --build
```

---

## ğŸ› Troubleshooting

### Problem: "ollama: connection refused"
**Ursache:** Ollama nicht verfÃ¼gbar  
**LÃ¶sung:**
```bash
docker ps  # Ist ollama am Laufen?
docker logs jarvis-ollama  # Logs prÃ¼fen
docker-compose up ollama  # Ollama einzeln starten
```

### Problem: "No space left on device"
**Ursache:** Docker hat nicht genug Platz  
**LÃ¶sung:**
```bash
# Alte Images lÃ¶schen
docker image prune -a

# Alte Volumes lÃ¶schen
docker volume prune

# Oder: Mehr Disk Space freigeben
```

### Problem: "Port 8080 already in use"
**Ursache:** Port 8080 wird bereits genutzt  
**LÃ¶sung:**
```yaml
# docker-compose.yml anpassen
jarvis-server:
  ports:
    - "8081:8080"  # Neuer Port 8081
```

### Problem: "Mistral model zu groÃŸ" (OOM)
**Ursache:** Nicht genug RAM (Mistral braucht ~8GB)  
**LÃ¶sung:**
```yaml
ollama:
  environment:
    - OLLAMA_DEFAULT_MODEL=tinyllama  # Kleineres Modell
```

### Problem: "Docker Build fehlgeschlagen"
**LÃ¶sung:**
```bash
# Dockerfile Ã¼berprÃ¼fen
cat Dockerfile

# Einzelne Layer debuggen
docker build --no-cache -t jarvis-test .

# Oder kompletter Rebuild
docker-compose down -v
docker-compose up --build
```

---

## ğŸ“ˆ Performance Optimierung

### 1. GPU-UnterstÃ¼tzung aktivieren

**NVIDIA GPU (falls vorhanden):**
```yaml
ollama:
  runtime: nvidia
  # oder
  deploy:
    resources:
      reservations:
        devices:
          - driver: nvidia
            count: 1
            capabilities: [gpu]
```

Installiere zuerst:
```bash
# NVIDIA Docker Runtime
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | \
  sudo tee /etc/apt/sources.list.d/nvidia-docker.list
sudo apt-get update && sudo apt-get install -y nvidia-docker2
sudo systemctl restart docker
```

### 2. Memory Limits

```yaml
jarvis-server:
  deploy:
    resources:
      limits:
        memory: 1G
      reservations:
        memory: 512M

ollama:
  deploy:
    resources:
      limits:
        memory: 8G
      reservations:
        memory: 6G
```

### 3. CPU Limits

```yaml
ollama:
  deploy:
    resources:
      limits:
        cpus: '4'
```

---

## ğŸŒ Multi-Node Deployment

Falls du auf mehreren Maschinen deployen mÃ¶chtest:

```bash
# Docker Swarm initialisieren
docker swarm init

# Andere Maschinen joinen
docker swarm join --token <token> <ip>:2377

# Deploy als Stack
docker stack deploy -c docker-compose.yml jarvis
```

---

## ğŸ“Š Ressourcen-Anforderungen

| Service | CPU | RAM | Disk |
|---------|-----|-----|------|
| Ollama (Mistral) | 2+ | 8GB | 10GB |
| JARVIS Server | 1 | 1GB | 100MB |
| **Gesamt** | **3+** | **9GB** | **10GB** |

---

## âœ… Checkliste fÃ¼r Production

- [ ] `docker-compose.yml` fÃ¼r Umgebung konfiguriert
- [ ] `ollama-init.sh` Shell-Script hat Execute-Permissions
- [ ] `Dockerfile` funktioniert (test mit `docker build`)
- [ ] `Dockerfile.ollama` funktioniert
- [ ] Logs konfiguriert (nicht zu verbose)
- [ ] Memory Limits gesetzt
- [ ] Restart-Policy: `unless-stopped`
- [ ] Volumes persistent & gesichert
- [ ] Ports korrekt mapped
- [ ] Environment-Variables sicher gespeichert
- [ ] Backups von Docker Volumes

---

## ğŸš€ Beispiel: Komplett aus Scratch

```bash
# 1. Repo klonen
git clone https://github.com/yourusername/jarvis.git
cd jarvis

# 2. Docker Compose starten
docker-compose up --build

# Warte bis beide Services online sind:
# âœ“ jarvis-ollama: "Listening on 0.0.0.0:11434"
# âœ“ jarvis-server: "Tomcat started on port 8080"

# 3. Browser Ã¶ffnen
open http://localhost:8080

# 4. Mikrofon erlauben & testen
# "JARVIS, wie heiÃŸt du?"

# 5. Fertig! ğŸ‰
```

---

## ğŸ”„ Continuous Deployment (CI/CD)

### GitHub Actions Example:

```yaml
name: Build and Push Docker Image

on:
  push:
    branches: [main]

jobs:
  build:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - uses: docker/setup-buildx-action@v1
      - uses: docker/build-push-action@v2
        with:
          context: .
          push: true
          tags: yourusername/jarvis:latest
```

---

## ğŸ“ Support

### Schnelle Hilfe:
```bash
docker-compose logs -f  # Live Logs
docker ps              # Running Containers
docker images          # VerfÃ¼gbare Images
```

### Detailliert:
Siehe `VOICE_ACTIVATION_GUIDE.md` und `JARVIS_TTS_SETUP.md`

---

## ğŸ‰ Ergebnis

**JARVIS lÃ¤uft jetzt vollstÃ¤ndig in Docker!**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Browser   â”‚ â† http://localhost:8080
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚ WebSocket
       â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   JARVIS Server      â”‚
â”‚  (Java/Spring Boot)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚ REST/gRPC
           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Ollama + Mistral  â”‚ â† port 11434
â”‚     (LLM Backend)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Viel Erfolg mit deinem JARVIS Docker Deployment! ğŸ³ğŸ¤–**

